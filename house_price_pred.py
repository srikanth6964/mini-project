# -*- coding: utf-8 -*-
"""House Price Pred

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L3qxtUEUDg8MbgRM3hPXoB20xtKXjpqa

LIBRARIES
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report,confusion_matrix
import warnings
import pickle
from scipy import stats
warnings.filterwarnings('ignore')
plt.style.use('fivethirtyeight')

"""READ THE DATASET"""

dt=pd.read_csv('/content/drive/MyDrive/ dataset/real estate valuation data set.csv')

dt.head()

dt

from google.colab import drive
drive.mount('/content/drive')

"""CHECKING NULL VALUES"""

dt.info()

dt.dropna(inplace=True)

dt.info()

dt.isnull().any()

x=dt.drop('Y house price of unit area',axis=1)
y=dt['Y house price of unit area']

"""UNIVARIATE"""

sns.displot(dt['X1 transaction date'])
sns.displot(dt['X2 house age'])

"""TRAIN THE DATA"""

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
train_dt=x_train.join(y_train)

train_dt

"""BIVARIATE"""

plt.figure(figsize=(16, 8))
sns.scatterplot(x='X3 distance to the nearest MRT station',
y='X4 number of convenience stores',
              data=train_dt,
                hue='Y house price of unit area',palette="coolwarm")
plt.title('Scatterplot of X3 distance to the nearest MRT station vs X4 number of convenience stores')
plt.show()

"""MULTIVARIATE"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Sample data
data = {
    "X1 transaction date": [2012.917, 2012.917, 2013.583, 2013.500, 2012.833],
    "X2 house age": [32.0, 19.5, 13.3, 13.3, 5.0],
    "X3 distance to the nearest MRT station": [84.87882, 306.59470, 561.98450, 561.98450, 390.56840],
    "X4 number of convenience stores": [10, 9, 5, 5, 5],
    "X5 latitude": [24.98298, 24.98034, 24.98746, 24.98746, 24.97937],
    "X6 longitude": [121.54024, 121.53951, 121.54391, 121.54391, 121.54245],
    "Y_house_price_of_unit_area": [37.9, 42.2, 47.3, 54.8, 43.1]
}
dt = pd.DataFrame(data)
plt.figure(figsize=(10, 6))
sns.countplot(x='Y_house_price_of_unit_area', hue='X4 number of convenience stores', data=dt)
plt.title('Count Plot of Y_house_price_of_unit_area by X4 number of convenience stores')
plt.xlabel('Y house price of unit area')
plt.ylabel('Count')
plt.legend(title='X4 number of convenience stores')
plt.show()

dt.describe()

dt.isnull().any()

"""OUTLIERS"""

dt.columns = dt.columns.str.replace(' ', '_')
column_to_check=['X1_transaction_date','X2_house_age','X3_distance_to_the_nearest_MRT_station','X4_number_of_convenience_stores','X5_latitude','X6_longitude','Y_house_price_of_unit_area']
outliers_dt=pd.DataFrame()
for column_name in column_to_check:
  Q1=dt['X3_distance_to_the_nearest_MRT_station'].quantile(0.05)
  Q3=dt['X3_distance_to_the_nearest_MRT_station'].quantile(0.95)
  IQR=Q3-Q1
  lower_limit=Q1-1.5*IQR
  upper_limit=Q3+1.5*IQR
  column_outliers=dt[(dt[column_name]<lower_limit)|(dt[column_name]>upper_limit)] # Now this line should work without error.
  column_outliers['Outlier_Column']=column_name
  outliers_dt=pd.concat([outliers_dt,column_outliers])
  outliers_dt.reset_index(drop=True,inplace=True)
  print("outliers in the column '{}':".format(column_name))
  print("All Outliers:")
  print(outliers_dt)
  plt.figure(figsize=(10,6))
  sns.boxplot(dt[column_name])
  plt.title('Boxplot of {}'.format(column_name))
  plt.tight_layout()
  plt.show()

"""HANDLING OUTLIERS"""

data = {
    "X1 transaction date": [2012.917, 2012.917, 2013.583, 2013.500, 2012.833],
    "X2 house age": [32.0, 19.5, 13.3, 13.3, 5.0],
    "X3 distance to the nearest MRT station": [84.87882, 306.59470, 561.98450, 561.98450, 390.56840],
    "X4 number of convenience stores": [10, 9, 5, 5, 5],
    "X5 latitude": [24.98298, 24.98034, 24.98746, 24.98746, 24.97937],
    "X6 longitude": [121.54024, 121.53951, 121.54391, 121.54391, 121.54245],
    "Y house price of unit area": [37.9, 42.2, 47.3, 54.8, 43.1]
}
dt = pd.DataFrame(data)

# Define a function to remove outliers based on IQR
def remove_outliers_iqr(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    return df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

# Remove outliers
dt_no_outliers = remove_outliers_iqr(dt)

print("Original dataset size:", dt.shape)
print("Dataset size after removing outliers:", dt_no_outliers.shape)
print("Number of rows removed:", dt.shape[0] - dt_no_outliers.shape[0])

sns.boxplot(data=dt,x='X3 distance to the nearest MRT station')

sns.boxplot(data=dt,x='X4 number of convenience stores')

sns.boxplot(data=dt,x='X5 latitude')

sns.boxplot(data=dt,x='X6 longitude')

sns.boxplot(data=dt,x='Y house price of unit area')

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

train_dt=x_train.join(y_train)
train_dt

X=dt.drop('Y house price of unit area',axis=1)
Y=dt['Y house price of unit area']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

train_dt=x_train.join(y_train)
train_dt

"""HISTOGRAM"""

train_dt.hist(figsize=(15,8))
plt.show()

"""HEATMAP"""

plt.figure(figsize=(12,8))
sns.heatmap(train_dt.corr(),annot=True)

train_dt['X1 transaction date'] = np.log(train_dt['X1 transaction date'])
train_dt['X2 house age'] = np.log(train_dt['X2 house age'])
train_dt['X3 distance to the nearest MRT station'] = np.log(train_dt['X3 distance to the nearest MRT station'])
train_dt['X4 number of convenience stores'] = np.log(train_dt['X4 number of convenience stores'])

train_dt

train_dt=pd.DataFrame(data)

train_dt.isna().any()

train_dt_cleaned=train_dt.dropna()

train_dt_filled=train_dt.fillna(train_dt.mean())

train_dt_cleaned.hist(figsize=(15,8))

train_dt_filled.hist(figsize=(15,8))

"""**HEAT MAP**"""

plt.figure(figsize=(12,8))
sns.heatmap(train_dt.corr(),annot=True)

x_train = x_train.dropna()
y_train = y_train.loc[x_train.index]

dt.dropna(inplace=True)

"""FEATURE MATRIX"""

X1_transaction_date=['2012.917','2012.917','2013.583','2013.500','2012.833']
X2_house_age=['32.0','19.5','13.3','13.3','5.0']
X3_distance_to_the_nearest_MRT_station=['84.87882','306.59470','561.98450','561.98450','390.56840']
X4_number_of_convenience_stores=['10','9','5','5','5']
X5_latitude=['24.98298','24.98034','24.98746','24.98746','24.97937']
X6_longitude=['121.54024','121.53951','121.54391','121.54391','121.54245']
dt = pd.DataFrame({
    'X1_transaction_date': X1_transaction_date,
    'X2_house_age': X2_house_age,
    'X3_distance_to_the_nearest_MRT_station': X3_distance_to_the_nearest_MRT_station,
    'X4_number_of_convenience_stores': X4_number_of_convenience_stores,
    'X5_latitude': X5_latitude,
    'X6_longitude': X6_longitude
})
numerical_features=dt[['X1_transaction_date','X2_house_age','X3_distance_to_the_nearest_MRT_station','X4_number_of_convenience_stores','X5_latitude','X6_longitude']]
X=dt.values
print("feature matrix")
print(X)

mean=train_dt['X3 distance to the nearest MRT station'].mean()
std=train_dt['X3 distance to the nearest MRT station'].std()
z_scores=[(x-mean)/std for x in train_dt['X3 distance to the nearest MRT station']]
print(z_scores)

x_train,y_train=train_dt.drop('Y house price of unit area',axis=1),train_dt['Y house price of unit area']

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=20)

"""REGRESSION MODELS"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# Assuming x_train, x_test, y_train are defined earlier
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# Initialize and fit the Linear Regression model
lr = LinearRegression()
lr.fit(x_train_scaled, y_train)

# Predict using the fitted model
y_pred_lr = lr.predict(x_test_scaled)

y_pred_lr

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Assuming x and y are your feature matrix and target vector respectively
# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=20)

# Initialize the Multiple Linear Regression model
mlr = LinearRegression()

# Train the model
mlr.fit(x_train, y_train)

# Make predictions on the test set
y_pred_mlr = mlr.predict(x_test)

from sklearn.tree import DecisionTreeRegressor
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=20)

dtr=DecisionTreeRegressor(random_state=20)
dtr.fit(x_train,y_train)

y_pred_dtr=dtr.predict(x_test)

from sklearn.ensemble import RandomForestRegressor
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=20)
rf=RandomForestRegressor(n_estimators=20,random_state=80)
rf.fit(x_train,y_train)

y_pred_rf=rf.predict(x_test)

import xgboost as xg
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=20)
xg=xg.XGBRegressor(objective='reg:linear',n_estimators=50,seed=23)

xg.fit(x_train,y_train)

y_pred_xg=xg.predict(x_test)

from sklearn.ensemble import GradientBoostingRegressor

gbr=GradientBoostingRegressor(n_estimators=18,max_depth=3,learning_rate=1)

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=20)

gbr.fit(x_train,y_train)

y_pred_gbr=gbr.predict(x_test)

from sklearn.ensemble import AdaBoostRegressor

adr=AdaBoostRegressor(n_estimators=10,learning_rate=1,random_state=20)
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=20)

adr.fit(x_train,y_train)

y_pred_adr=adr.predict(x_test)

"""COMPARE ALL MODELS"""

print("The accuracy of Linear Regression:",r2_score(y_pred_lr,y_test))
print("The accuracy of multilinear regression:",r2_score(y_pred_mlr,y_test))
print("The accuracy of Decision Tree Regression:",r2_score(y_pred_dtr,y_test))
print("The accuracy of Random Forest Regression",r2_score(y_pred_rf,y_test))
print("The accuracy of XGBoost Regression",r2_score(y_pred_xg,y_test))
print("The accuracy of Gradient Boosting Regression",r2_score(y_pred_gbr,y_test))
print("The accuracy of Adaboost Regression",r2_score(y_pred_adr,y_test))

from sklearn.ensemble import RandomForestRegressor
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
rf=RandomForestRegressor(n_estimators=20,random_state=80)

rf.fit(x_train,y_train)

y_pred_rf=rf.predict(x_test)

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
ac = r2_score(y_pred_rf, y_test)
rms = np.sqrt(mean_squared_error(y_pred_rf, y_test))
ms = mean_squared_error(y_pred_rf, y_test)
ac,rms,ms

from sklearn.preprocessing import StandardScaler

# Assuming you have your training data `train_data`
sc = StandardScaler()
sc.fit(train_dt)  # Fit the scaler to your training data

"""DOWNLOAD PICKLE"""

import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
rf_model = RandomForestRegressor()
scaler = StandardScaler()
with open('price.pkl', 'wb') as f:
    pickle.dump(rf_model, f)
with open('scale.pkl', 'wb') as f:
    pickle.dump(scaler, f)

from google.colab import files
files.download('price.pkl')

from google.colab import files
files.download('scale.pkl')

from google.colab import files
files.download('/content/drive/MyDrive/ dataset/real estate valuation data set.csv')